{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro-loka/.cache/pypoetry/virtualenvs/yt-summaries-019raoQi-py3.10/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/home/pedro-loka/.cache/pypoetry/virtualenvs/yt-summaries-019raoQi-py3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from ast import literal_eval\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "from yt_rag.agent import llm, build_prompt\n",
    "from yt_rag.build_index import build_index, create_embeddings, embed_title\n",
    "from yt_info.yt_video_data import Video, get_video_transcript\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OLLAMA_URL = os.getenv(\"LOCAL_OLLAMA_URL\")\n",
    "CHANNELS = literal_eval(os.getenv(\"YT_CHANNELS\"))\n",
    "ES_URL = os.getenv(\"LOCAL_ES_URL\")\n",
    "ES_INDEX_NAME = os.getenv(\"ES_INDEX_NAME\")\n",
    "\n",
    "ollama_client = OpenAI(base_url=OLLAMA_URL, api_key=\"ollama\")\n",
    "embedding_model = SentenceTransformer(\"multi-qa-distilbert-cos-v1\")\n",
    "\n",
    "es_client = Elasticsearch(ES_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/yt_videos_details.pkl', 'rb') as f:\n",
    "    videos = pickle.load(f)\n",
    "\n",
    "\n",
    "# change to results_tempDefault.pkl to use the LLM generated questions with the default temperature.\n",
    "with open('../data/results_temp0.pkl', 'rb') as f:\n",
    "    results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_llm = \"\"\"\n",
    "You are a user of a cooking and recipe assistant app.\n",
    "Generate 5 general questions related to the cooking techniques, ingredients, or recipes discussed in the provided video.\n",
    "The questions should be clear, concise, and relevant to the video's content.\n",
    "Avoid being overly specific about particular ingredients or using too many details from the title and description.\n",
    "Do not mention or reference the video title, description, or any URLs.\n",
    "Focus on broader cooking themes that would be of interest to a home cook.\n",
    "\n",
    "The video information:\n",
    "\n",
    "title: {title}\n",
    "description: {description}\n",
    "\n",
    "Output the result as a JSON object without using code blocks:\n",
    "\n",
    "{{\"questions\": [\"question1\", \"question2\", \"question3\", \"question4\", \"question5\"]}}\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(video):\n",
    "    prompt = prompt_llm.format(**video.__dict__)\n",
    "\n",
    "    response = llm(prompt, client=ollama_client)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "failed_video_ids = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for video in tqdm(videos): \n",
    "    if video.video_id in results:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        questions_raw = generate_questions(video)\n",
    "    except Exception as err:\n",
    "        print(f\"Failed {video.video_id}\", err)\n",
    "        continue\n",
    "\n",
    "\n",
    "    try:\n",
    "        questions = json.loads(questions_raw.replace(\"json\", \"\").replace(\"`\", \"\").strip())\n",
    "    except json.JSONDecodeError:\n",
    "        # print(f\"JSON fail for {video.video_id}\")\n",
    "        failed_video_ids[video.video_id] = questions_raw\n",
    "        continue        \n",
    "    except Exception as err:\n",
    "        # print(f\"Failed {video.video_id}\", err)\n",
    "        failed_video_ids[video.video_id] = questions_raw\n",
    "        continue\n",
    "    else:\n",
    "        \n",
    "        try:\n",
    "            results[video.video_id] = questions['questions']\n",
    "        except KeyError:\n",
    "            failed_video_ids[video.video_id] = questions_raw\n",
    "            continue\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elastic_search_text(query):\n",
    "    search_query = {\n",
    "        \"_source\": [\"title\", \"is_short\", \"description\", \"video_id\"],\n",
    "        \"size\": 5,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": {\n",
    "                    \"multi_match\": {\n",
    "                        \"query\": query,\n",
    "                        \"fields\": [\"title\", \"description\"],\n",
    "                        \"type\": \"best_fields\",\n",
    "                    }\n",
    "                },\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "\n",
    "    response = es_client.search(index=ES_INDEX_NAME, body=search_query)\n",
    "    return [hit[\"_source\"] for hit in response[\"hits\"][\"hits\"]]\n",
    "\n",
    "\n",
    "def elastic_search_knn(query, index=ES_INDEX_NAME, field=\"title_description_vector\"):\n",
    "        \n",
    "    vector = embedding_model.encode(query)\n",
    "    knn = {\n",
    "        \"field\": field,\n",
    "        \"query_vector\": vector,\n",
    "        \"k\": 5,\n",
    "        \"num_candidates\": 600,\n",
    "    }\n",
    "\n",
    "    search_query = {\n",
    "        \"knn\": knn,\n",
    "        \"_source\": [\"title\", \"is_short\", \"description\", \"course\", \"video_id\"],\n",
    "    }\n",
    "    es_results = es_client.search(index=index, body=search_query)\n",
    "\n",
    "    result_docs = []\n",
    "\n",
    "    for hit in es_results[\"hits\"][\"hits\"]:\n",
    "        result_docs.append(hit[\"_source\"])\n",
    "\n",
    "    return result_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hit_rate(relevance_total):\n",
    "    cnt = 0\n",
    "\n",
    "    for line in relevance_total:\n",
    "        if True in line:\n",
    "            cnt = cnt + 1\n",
    "\n",
    "    return cnt / len(relevance_total)\n",
    "\n",
    "def mrr(relevance_total):\n",
    "    total_score = 0.0\n",
    "\n",
    "    for line in relevance_total:\n",
    "        for rank in range(len(line)):\n",
    "            if line[rank] == True:\n",
    "                total_score = total_score + 1 / (rank + 1)\n",
    "\n",
    "    return total_score / len(relevance_total)\n",
    "\n",
    "def evaluate(ground_truth, search_function):\n",
    "    relevance_total = []\n",
    "\n",
    "    for q in tqdm(ground_truth):\n",
    "        doc_id = q['id']\n",
    "        results = search_function(q)\n",
    "        relevance = [d['video_id'] == doc_id for d in results]\n",
    "        relevance_total.append(relevance)\n",
    "\n",
    "    return {\n",
    "        'hit_rate': hit_rate(relevance_total),\n",
    "        'mrr': mrr(relevance_total),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = [{\"id\": k, \"question\": _v} for k, v in results.items() for _v in v]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector+KNN search CONCAT(Title + description) with Phi3-Mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting embedding...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c22e08e7857044f6b2bfcca0b8a7e8c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/475 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...embedding done.\n",
      "Started indexing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d11ec8277ede47c68a63f04aa3205705",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/475 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...indexing done.\n"
     ]
    }
   ],
   "source": [
    "embeddings = create_embeddings(videos)\n",
    "build_index(videos, embeddings, ES_INDEX_NAME, es_client=es_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2041/2041 [01:39<00:00, 20.54it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'hit_rate': 0.6281234688878001, 'mrr': 0.5037563285970942}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(ground_truth, lambda q: elastic_search_knn(q['question']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text search on Title and description with Phi3-Mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2041/2041 [00:09<00:00, 213.36it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'hit_rate': 0.4737873591376776, 'mrr': 0.3781724644781975}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(ground_truth, lambda q: elastic_search_text(q['question']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector+KNN search on Title with Phi3-Mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting embedding...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "804ae5a345e14d71a74b34cda0178cb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/475 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...embedding done.\n",
      "Started indexing...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "892448d737ef41eca0fdc1453182c1f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/475 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...indexing done.\n"
     ]
    }
   ],
   "source": [
    "title_only_index_name = ES_INDEX_NAME+\"-title-only\"\n",
    "embeddings = create_embeddings(videos, embedding_function=embed_title)\n",
    "build_index(videos, embeddings, title_only_index_name, es_client=es_client, field=\"title_vector\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2041/2041 [01:00<00:00, 33.81it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'hit_rate': 0.581577658010779, 'mrr': 0.46641352278295073}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(ground_truth, lambda q: elastic_search_knn(q['question'], index=title_only_index_name, field=\"title_vector\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_rag_eval = \"\"\"\n",
    "You are an expert evaluator for a RAG system.\n",
    "Your task is to analyze the relevance of the generated answer to the given question.\n",
    "Based on the relevance of the generated answer, you will classify it\n",
    "as \"NON_RELEVANT\", \"PARTLY_RELEVANT\", or \"RELEVANT\".\n",
    "\n",
    "Here is the data for evaluation:\n",
    "\n",
    "Question: {question}\n",
    "Generated Answer: {answer_llm}\n",
    "\n",
    "Please analyze the content and context of the generated answer in relation to the question\n",
    "and provide your evaluation in parsable JSON without using code blocks:\n",
    "\n",
    "{{\n",
    "  \"Relevance\": \"NON_RELEVANT\" | \"PARTLY_RELEVANT\" | \"RELEVANT\",\n",
    "  \"Explanation\": \"[Provide a brief explanation for your evaluation]\"\n",
    "}}\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer(query):\n",
    "    es_videos = elastic_search_knn(query)\n",
    "    es_videos = [Video(**video) for video in es_videos]\n",
    "    transcripts = []\n",
    "    for video in es_videos[:1]:\n",
    "        transcript = get_video_transcript(video)\n",
    "        transcripts.append(\"\\n\".join([line[\"text\"] for line in transcript]))\n",
    "\n",
    "    prompt = build_prompt(query, es_videos[:1], transcripts)\n",
    "    answer = llm(prompt, client=ollama_client)\n",
    "\n",
    "    return answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = random.sample(ground_truth, 200)\n",
    "rag_evals = []\n",
    "for entry in tqdm(sample):\n",
    "    question = entry[\"question\"]\n",
    "    \n",
    "    llm_answer = get_answer(question)\n",
    "    rag_eval_prompt = prompt_rag_eval.format(question=question, answer_llm=llm_answer)   \n",
    "\n",
    "    rag_llm_eval = llm(rag_eval_prompt, client=ollama_client)\n",
    "    \n",
    "    rag_evals.append((entry, rag_llm_eval))\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[i for i in videos if i.video_id == \"vaX2Dv6TaSk\"]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "42097c99d85c3de64a416612e20e99fe9f8b2589256f2e83a2a3abfd51f5773a"
  },
  "kernelspec": {
   "display_name": "Python 3.10.12 ('yt-summaries-019raoQi-py3.10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
