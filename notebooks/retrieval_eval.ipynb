{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro-loka/.cache/pypoetry/virtualenvs/yt-summaries-019raoQi-py3.10/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n",
      "/home/pedro-loka/.cache/pypoetry/virtualenvs/yt-summaries-019raoQi-py3.10/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from ast import literal_eval\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "from yt_rag.agent import llm, build_prompt\n",
    "from yt_rag.build_index import build_index, create_embeddings, embed_title\n",
    "from yt_info.yt_video_data import Video, get_video_transcript\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OLLAMA_URL = os.getenv(\"LOCAL_OLLAMA_URL\")\n",
    "CHANNELS = literal_eval(os.getenv(\"YT_CHANNELS\"))\n",
    "ES_URL = os.getenv(\"LOCAL_ES_URL\")\n",
    "ES_INDEX_NAME = os.getenv(\"ES_INDEX_NAME\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "ollama_client = OpenAI(base_url=OLLAMA_URL, api_key=\"ollama\")\n",
    "openai_client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "embedding_model = SentenceTransformer(\"multi-qa-distilbert-cos-v1\")\n",
    "\n",
    "es_client = Elasticsearch(ES_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/yt_videos_details.pkl', 'rb') as f:\n",
    "    videos = pickle.load(f)\n",
    "\n",
    "\n",
    "# change to results_tempDefault.pkl to use the LLM generated questions with the default temperature.\n",
    "with open('../data/results_temp0.pkl', 'rb') as f:\n",
    "    results = pickle.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elastic_search_text(query):\n",
    "    search_query = {\n",
    "        \"_source\": [\"title\", \"is_short\", \"description\", \"video_id\"],\n",
    "        \"size\": 5,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": {\n",
    "                    \"multi_match\": {\n",
    "                        \"query\": query,\n",
    "                        \"fields\": [\"title\", \"description\"],\n",
    "                        \"type\": \"best_fields\",\n",
    "                    }\n",
    "                },\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "\n",
    "    response = es_client.search(index=ES_INDEX_NAME, body=search_query)\n",
    "    return [hit[\"_source\"] for hit in response[\"hits\"][\"hits\"]]\n",
    "\n",
    "\n",
    "def elastic_search_knn(query, index=ES_INDEX_NAME, field=\"title_description_vector\"):\n",
    "        \n",
    "    vector = embedding_model.encode(query)\n",
    "    knn = {\n",
    "        \"field\": field,\n",
    "        \"query_vector\": vector,\n",
    "        \"k\": 5,\n",
    "        \"num_candidates\": 10000,\n",
    "    }\n",
    "\n",
    "    search_query = {\n",
    "        \"knn\": knn,\n",
    "        \"_source\": [\"title\", \"is_short\", \"description\", \"course\", \"video_id\"],\n",
    "    }\n",
    "    es_results = es_client.search(index=index, body=search_query)\n",
    "\n",
    "    result_docs = []\n",
    "\n",
    "    for hit in es_results[\"hits\"][\"hits\"]:\n",
    "        result_docs.append(hit[\"_source\"])\n",
    "\n",
    "    return result_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hit_rate(relevance_total):\n",
    "    cnt = 0\n",
    "\n",
    "    for line in relevance_total:\n",
    "        if True in line:\n",
    "            cnt = cnt + 1\n",
    "\n",
    "    return cnt / len(relevance_total)\n",
    "\n",
    "def mrr(relevance_total):\n",
    "    total_score = 0.0\n",
    "\n",
    "    for line in relevance_total:\n",
    "        for rank in range(len(line)):\n",
    "            if line[rank] == True:\n",
    "                total_score = total_score + 1 / (rank + 1)\n",
    "\n",
    "    return total_score / len(relevance_total)\n",
    "\n",
    "def evaluate(ground_truth, search_function):\n",
    "    relevance_total = []\n",
    "\n",
    "    for q in tqdm(ground_truth):\n",
    "        doc_id = q['id']\n",
    "        results = search_function(q)\n",
    "        relevance = [d['video_id'] == doc_id for d in results]\n",
    "        relevance_total.append(relevance)\n",
    "\n",
    "    return {\n",
    "        'hit_rate': hit_rate(relevance_total),\n",
    "        'mrr': mrr(relevance_total),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = [{\"id\": k, \"question\": _v} for k, v in results.items() for _v in v]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector+KNN search CONCAT(Title + description) with Phi3-Mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = create_embeddings(videos)\n",
    "build_index(videos, embeddings, ES_INDEX_NAME, es_client=es_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(ground_truth, lambda q: elastic_search_knn(q['question']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text search on Title and description with Phi3-Mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(ground_truth, lambda q: elastic_search_text(q['question']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector+KNN search on Title with Phi3-Mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_only_index_name = ES_INDEX_NAME+\"-title-only\"\n",
    "embeddings = create_embeddings(videos, embedding_function=embed_title)\n",
    "build_index(videos, embeddings, title_only_index_name, es_client=es_client, field=\"title_vector\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(ground_truth, lambda q: elastic_search_knn(q['question'], index=title_only_index_name, field=\"title_vector\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rrf(rank, k=60):\n",
    "    return 1 / (k + rank)\n",
    "\n",
    "def elastic_search_hybrid_rrf(query, index=ES_INDEX_NAME, field=\"title_description_vector\", k=60):\n",
    "    vector = embedding_model.encode(query)\n",
    "    knn = {\n",
    "        \"field\": field,\n",
    "        \"query_vector\": vector,\n",
    "        \"k\": 20,\n",
    "        \"num_candidates\": 10000,\n",
    "    }\n",
    "\n",
    "    knn_results = es_client.search(\n",
    "        index=index, \n",
    "        body={\n",
    "            \"knn\": knn, \n",
    "            \"size\": 20,\n",
    "            \"_source\": [\"title\", \"is_short\", \"description\", \"course\", \"video_id\"],\n",
    "        }\n",
    "    )['hits']['hits']\n",
    "    \n",
    "    kw_query = {\n",
    "        \"_source\": [\"title\", \"is_short\", \"description\", \"video_id\"],\n",
    "        \"size\": 5,\n",
    "        \"query\": {\n",
    "            \"bool\": {\n",
    "                \"must\": {\n",
    "                    \"multi_match\": {\n",
    "                        \"query\": query,\n",
    "                        \"fields\": [\"title\", \"description\"],\n",
    "                        \"type\": \"best_fields\",\n",
    "                    }\n",
    "                },\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "\n",
    "    kw_results = es_client.search(index=ES_INDEX_NAME, body=kw_query)[\"hits\"][\"hits\"]\n",
    "    \n",
    "    rrf_scores = {}\n",
    "    # Calculate RRF using vector search results\n",
    "    for rank, hit in enumerate(knn_results):\n",
    "        doc_id = hit['_id']\n",
    "        rrf_scores[doc_id] = compute_rrf(rank + 1, k)\n",
    "\n",
    "        # Adding keyword search result scores\n",
    "    for rank, hit in enumerate(kw_results):\n",
    "        doc_id = hit['_id']\n",
    "        if doc_id in rrf_scores:\n",
    "            rrf_scores[doc_id] += compute_rrf(rank + 1, k)\n",
    "        else:\n",
    "            rrf_scores[doc_id] = compute_rrf(rank + 1, k)\n",
    "\n",
    "    # Sort RRF scores in descending order\n",
    "    reranked_docs = sorted(rrf_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Get top-K documents by the score\n",
    "    final_results = []\n",
    "    for doc_id, score in reranked_docs[:5]:\n",
    "        doc = es_client.get(index=index, id=doc_id)\n",
    "        final_results.append(doc['_source'])\n",
    "    \n",
    "    return final_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2041/2041 [01:56<00:00, 17.55it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'hit_rate': 0.6325330720235178, 'mrr': 0.5082149273232087}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def question_text_hybrid_rrf(q):\n",
    "    question = q['question']\n",
    "\n",
    "    return elastic_search_hybrid_rrf(question)\n",
    "\n",
    "evaluate(ground_truth, question_text_hybrid_rrf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_rag_eval = \"\"\"\n",
    "You are an expert evaluator for a RAG system.\n",
    "Your task is to analyze the relevance of the generated answer to the given question.\n",
    "Based on the relevance of the generated answer, you will classify it\n",
    "as \"NON_RELEVANT\", \"PARTLY_RELEVANT\", or \"RELEVANT\".\n",
    "\n",
    "Here is the data for evaluation:\n",
    "\n",
    "Question: {question}\n",
    "Generated Answer: {answer_llm}\n",
    "\n",
    "Please analyze the content and context of the generated answer in relation to the question\n",
    "and provide your evaluation in parsable JSON without using code blocks:\n",
    "\n",
    "{{\n",
    "  \"Relevance\": \"NON_RELEVANT\" | \"PARTLY_RELEVANT\" | \"RELEVANT\",\n",
    "  \"Explanation\": \"[Provide a brief explanation for your evaluation]\"\n",
    "}}\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template1 = \"\"\"\n",
    "You are a professional cook and recipe developer. Answer the QUESTION using only the information provided in the CONTEXT from the video transcript.\n",
    "Do not include any information, assumptions, or details not present in the CONTEXT. If the CONTEXT does not provide enough information to answer the QUESTION, acknowledge the limitation.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\"\"\".strip()\n",
    "\n",
    "prompt_template2 = \"\"\"\n",
    "You are a professional cook and recipe developer. Your task is to answer the QUESTION based only on the information provided in the CONTEXT from the video transcript.\n",
    "\n",
    "Please follow these guidelines:\n",
    "- Provide a clear and concise answer directly related to the QUESTION.\n",
    "- Avoid any conversational elements or references to the video format (such as \"welcome back\" or \"subscribe\").\n",
    "- Do not introduce information, assumptions, or details that are not present in the CONTEXT.\n",
    "- If the CONTEXT does not provide enough information to fully answer the QUESTION, acknowledge this and explain the limitation.\n",
    "\n",
    "QUESTION: {question}\n",
    "\n",
    "CONTEXT:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "def get_answer(query, prompt_template):\n",
    "    es_videos = elastic_search_knn(query)\n",
    "    es_videos = [Video(**video) for video in es_videos]\n",
    "    transcripts = []\n",
    "    for video in es_videos[:1]:\n",
    "        transcript = get_video_transcript(video)\n",
    "        transcripts.append(\"\\n\".join([line[\"text\"] for line in transcript]))\n",
    "\n",
    "    prompt = build_prompt(query, es_videos[:1], transcripts, prompt_template=prompt_template)\n",
    "    answer = llm(prompt, model_choice=\"ollama/phi3:mini\")\n",
    "    \n",
    "\n",
    "    return answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = random.sample(ground_truth, 100)\n",
    "rag_evals_template_1 = []\n",
    "rag_evals_template_2 = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in tqdm(sample[7:]):\n",
    "    question = entry[\"question\"]\n",
    "    \n",
    "    llm_answer_template1 = get_answer(question, prompt_template1)\n",
    "    llm_answer_template2 = get_answer(question, prompt_template2)\n",
    "    \n",
    "    rag_eval_prompt_template1 = prompt_rag_eval.format(question=question, answer_llm=llm_answer_template1)   \n",
    "    rag_eval_prompt_template2 = prompt_rag_eval.format(question=question, answer_llm=llm_answer_template2)   \n",
    "\n",
    "    rag_llm_eval_template1 = llm(rag_eval_prompt_template1, model_choice=\"openai/gpt-4o-mini\")\n",
    "    rag_llm_eval_template2 = llm(rag_eval_prompt_template2, model_choice=\"openai/gpt-4o-mini\")\n",
    "    \n",
    "    rag_evals_template_1.append((entry, rag_llm_eval_template1))\n",
    "    rag_evals_template_2.append((entry, rag_llm_eval_template2))\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "42097c99d85c3de64a416612e20e99fe9f8b2589256f2e83a2a3abfd51f5773a"
  },
  "kernelspec": {
   "display_name": "Python 3.10.12 ('yt-summaries-019raoQi-py3.10')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
